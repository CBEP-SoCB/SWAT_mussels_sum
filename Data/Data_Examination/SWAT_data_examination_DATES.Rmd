---
title: "Review of Maine DEP EGAD Mussel Tissue Toxics Data Sampling Dates"
subtitle: "Review of Sample Dates"
author: "Curtis C. Bohlen, Casco Bay Estuary Partnership"
date: "4/01/2021"
output:
  github_document:
    toc: true
    toc_depth: 2
    fig_width: 7
    fig_height: 5
---

<img
  src="https://www.cascobayestuary.org/wp-content/uploads/2014/04/logo_sm.jpg"
  style="position:absolute;top:10px;right:50px;" />


# Introduction
Maine's Department of Environmental Protection (DEP) maintains a large database
of environmental data called "EGAD".  Citizens can request data from the
database through DEP staff.

CBEP requested data from DEP on levels of toxic contaminants in shellfish
tissue samples from Casco Bay. The result is a large (> 100,000 line) excel
spreadsheet containing data from about 40 sampling dates from 20 locations, over
a period of more than 15 years.

Unfortunately, the data delivery contains little metadata, so it takes some
effort to understand the data format and analyze it correctly. Among other
problems, we need to understand dates and locations of samples, what analytes
were used for different samples, etc.

In this notebook and accompanying notebooks, we take various slices through the
data to understand its structure.

# Load Libraries
```{r load_libraries}
library(tidyverse)
library(readxl)
library(htmltools)  # used by knitr called here only to avoid startup text later in document
library(knitr)
```
# Load Data
## Establish Folder References
```{r folder_refs}
auntfldnm <- 'Original_Data'
parent   <- dirname(getwd())
grandparent <- dirname(parent)
aunt  <- file.path(grandparent,auntfldnm)
fn <- 'CascoBaySWATtissue_Bohlen.xlsx'
```

## Copy Data
This is a larger data file that takes some time to load.  Getting the column
types right dramatically improves load speed. Much of the data is qualitative,
and can't be handled in R.
```{r copy_data}
SWAT_data <- read_excel(file.path(aunt, fn),
    sheet = "Mussels Data", col_types = c("numeric", 
        "text", "text", "text", "text", "text", 
        "text", "text", "text", "text", "text", 
        "text", "date", "text", "text", 
        "text", "date", "text", "numeric", 
        "text", "text", "text", "text", 
        "text", "numeric", "numeric", "text", 
        "text", "text", "text", "text", 
        "text", "numeric", "text", 
        "text", "text", "text", "text", 
        "text", "text", "text"))

before <- nrow(SWAT_data)
```

## Remove Duplicates
Many samples -- nearly 20% -- are members of a group of duplicates.  We can
think of no valid reason why two records should be exact duplicates in this
setting, so we remove all duplicates using the unique() function.

```{r remove_duplicates}
SWAT_data <- unique(SWAT_data)
```

# Sampling Dates and Sites
```{r dates_data}
dates_data <- SWAT_data %>%
  select(SAMPLE_DATE, `EGAD_SITE_NAME`) %>%
  group_by(`EGAD_SITE_NAME`, `SAMPLE_DATE`) %>%
  summarize(Code =  first(sub('.* - ','', `EGAD_SITE_NAME`)), 
            Site =  first(sub(' - .*','', `EGAD_SITE_NAME`)),
            Year =  first(as.numeric(format(SAMPLE_DATE,'%Y'))),
                        .groups = 'drop') %>%
  rename(Date = SAMPLE_DATE) %>%
  select(Code, Site, Year, Date) %>%
  arrange(Year, Date)
kable(dates_data)
```

That Suggests we have FORTY unique sampling SAMPLE_DATEs and SITEs Note that
there are several times that we have multiple sites sampled in a given date, and
several times that the same site is sampled multiple times in the same year.

## How Many Times has Each Site Been Sampled?
```{r}
dates_data %>%
  group_by(Site, Year) %>%
  mutate(count = n())  %>%
  ungroup() %>%
  group_by(Site) %>%
  summarize(spread = range(Year)[[2]]-range(Year)[[1]] + 1,
            nsamps = n(),
            maxsamps = max(count),
            .groups = 'drop') %>%
  kable(col.names = c('Site',
                      'Period between First and Last Samples (Years)',
                      'Total Number of Sampling Events',
                      'Maximum Number of Samples in One Year'))
```

So, there are basically TWO sites which have been sampled fairly regularly,
and a handful of sites sampled more than twice.  Trend analysis
may be possible looking at:

*  Mare Brook      (three times over ten years)  
*  East End Beach  (six times over eleven years)  
*  Mill Creek      (Six Times over twelve years)  
*  Spring Point    (four times over nine years) 

Notice that the "Fore River Outer" site was sampled three times in one year.

## Period of Data 
```{r}
range(dates_data$Year)
```

## How Many Sampling Events Each Year?
```{r}
dates_data %>%
  group_by(Year,Date) %>%
  summarize(Date = first(Date), .groups = 'drop_last') %>%
  summarize(count = n())
```


## Look at Recent Data 
Recent data is since 2010.
## How Many Times has Each Site Been Sampled?
```{r}
dates_data %>%
  filter(Year > 2009) %>%
  group_by(Site, Year) %>%
  mutate(count = n())  %>%
  ungroup() %>%
  group_by(Site) %>%
  summarize(spread = range(Year)[[2]]-range(Year)[[1]] + 1,
            nsamps = n(),
            maxsamps = max(count),
            .groups = 'drop') %>%
  kable(col.names = c('Site',
                      'Period between First and Last Samples (Years)',
                      'Total Number of Sampling Events',
                      'Maximum Number of Samples in One Year'))
```
