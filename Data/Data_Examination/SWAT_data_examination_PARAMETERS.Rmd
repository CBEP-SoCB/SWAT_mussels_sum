---
title: "Review of Maine DEP EGAD Mussel Tissue Toxics Parameters"
subtitle: "Identifying and Classifying Parameters"
author: "Curtis C. Bohlen, Casco Bay Estuary Partnership"
date: "9/18/2020"
output:
  github_document:
    toc: true
    toc_depth: 2
    fig_width: 7
    fig_height: 5
---

<img
  src="https://www.cascobayestuary.org/wp-content/uploads/2014/04/logo_sm.jpg"
  style="position:absolute;top:10px;right:50px;" />

# Introduction
Maine's Department of Environmental Protection (DEP) maintains a large database
of environmental data called "EGAD".  Citizens can request data from the
database through DEP staff.

CBEP requested data from DEP on levels of toxic contaminants in shellfish
tissue samples from Casco Bay. The result is a large (> 100,000 line) excel
spreadsheet containing data from about 40 sampling dates from 20 locations, over
a period of more than 15 years.

Unfortunately, the data delivery contains limited metadata, so it takes some
effort to understand the data format and analyze it correctly.

In this notebook we review the parameters identified in the data and assign them
to groups to facilitate analysis.  We also check on a number of technical
matters dealing with how various totals were calculated.  These include:
*  Figuring out which compounds were included in various totals
*  Determining how NOn-detects were included in totals

# Load Libraries
```{r load_libraries}
library(tidyverse)
library(readxl)
library(htmltools)  # used by knitr called here only to avoid startup text later
library(knitr)
```

# Load Data
## Establish Folder Reference
```{r folder_refs}
auntfldnm <- 'Original_Data'
parent   <- dirname(getwd())
grandparent <- dirname(parent)
aunt  <- file.path(grandparent,auntfldnm)
fn <- 'CascoBaySWATtissue_Bohlen.xlsx'
```

## Copy Data
This is a larger data file that takes some time to load.  Getting the column
types right dramatically improves load speed. Much of the data is qualitative,
and can't be handled in R.
```{r copy_data}
SWAT_data <- read_excel(file.path(aunt, fn), 
    sheet = "Mussels Data", col_types = c("numeric", 
        "text", "text", "text", "text", "text", 
        "text", "text", "text", "text", "text", 
        "text", "date", "text", "text", 
        "text", "date", "text", "numeric", 
        "text", "text", "text", "text", 
        "text", "numeric", "numeric", "text", 
        "text", "text", "text", "text", 
        "text", "numeric", "text", 
        "text", "text", "text", "text", 
        "text", "text", "text"))

before <- nrow(SWAT_data)
```

## Remove duplicates
Many samples -- nearly 20% -- are members of a group of duplicates.  We can
think of no valid reason why two records should be exact duplicates in this
setting, so we remove all duplicates using the unique() function.
```{r remove_duplicates}
SWAT_data <- unique(SWAT_data)
```

## Simplify Data and Add Unique Sample Codes
This logic was developed in "SWAT_data examination_UNIQUE.Rmd".
```{r}
SWAT_simplified <- SWAT_data %>%
  # Eliminate uninformative identifiers
  select    (-`SAMPLE TYPE`, -`SAMPLE POINT TYPE`, -`SAMPLE LOCATION`,
             -`RESULT TYPE`, -`PARAMETER_QUALIFIER`, -`PARAMETER FILTERED`,
             -`SAMPLE FILTER`, -`DEPTH`, -`DEPTH UNITS`,
             -TREATMENT, -`METER_CALIBRATED`) %>%
  
  # Eliminate data we will not analyze
  select    (-SITE_DESCRIPTION, -ANALYSIS_DATE,
             -`QC TYPE`, -SAMPLED_BY, -`UNITS DESCRIPTION`,
             -`SAMPLE COMMENT`, -`LAB COMMENT`, -`VALIDATION COMMENT`) %>%
  
  # Create Site Code and Site Name
  mutate    (SiteCode =  sub('.* - ','', `EGAD_SITE_NAME`), 
             Site     =  sub(' - .*','', `EGAD_SITE_NAME`)) %>%
  select    (-EGAD_SITE_NAME) %>%
  
  # Create Year Time Stamp and (Draft 1) Unique Sample ID
  mutate    (Year  = as.numeric(format(SAMPLE_DATE, '%Y')),
             sample_id = gsub(" ", "_", SAMPLE_ID)) %>%
  group_by  (Year) %>%
  mutate    (tag = as.numeric(factor(SAMPLE_DATE))) %>%
  ungroup   ()  %>%
  mutate    (Code = paste(sample_id, Year, tag, sep = '_')) %>%
  select    (-sample_id, -tag) %>%
  select    (`SITE SEQ`, SiteCode, Site, Year, SAMPLE_DATE,
              SAMPLE_ID, Code, everything())
```

# How to Group Parameters?
One of the challenges is that we want to report totals of various groups of
parameters, but the data contains no flag for major groups of parameters.

The closest thing to an indicator of the groupings of parameters in use is the
`TEST METHOD` field. We generated a pretty good starting point for a lookup
table to assign parameters to groups based on the `TEST METHOD`.

We then exported that list to Excel, and edit it by hand to assign parameters to 
groups.


## Develop a Preliminary list of Parameters
```{r}
SWAT_simplified %>%
  select(PARAMETER, `TEST METHOD`) %>%
  group_by(`TEST METHOD`, PARAMETER) %>%
  summarize(Test      =  first(`TEST METHOD`),
                        .groups = 'drop') %>%
  rename(Parameter = PARAMETER) %>%
  select(-Test) %>%
  kable()
```
Some of the PAH values marked as "Calculated", especially the groups of PAHs
designated  with a "C-" value may not always represent calculations. In 
standard practice, those values are aggregates of substituted PAHs, either
calculated or not. Not all the subcomponent PAHs for these sums appear in the
data, so it is not clear that these are all simple calculations rather than
data in their own right.

##  Read Excel File with Classification of Parameters
We loaded a version of the preceding table into Excel, and hand edited it to
provide useful groupings of parameters.  here we read that list back into R.
```{r}
Parameter_List <- read_excel(file.path(parent,"Parameter List.xlsx"), 
    sheet = "Parameter List") %>%
  mutate(Class = factor(Class)) %>%
  arrange(Class, PARAMETER)
Parameter_List
```

## Add Class to the Working Simplified Data
```{r}
SWAT_simplified <- SWAT_simplified %>% 
  mutate(Class = Parameter_List$Class[match(PARAMETER,
                                            Parameter_List$PARAMETER)])
```

## How are contaminant classes distributed among sampling events?
```{r}
SWAT_simplified %>% 
  group_by(SiteCode, SAMPLE_DATE, Class) %>%
  mutate(class = factor(Class)) %>%
  summarize(n = sum(! is.na(CONCENTRATION)),
            .groups = 'drop') %>%
  pivot_wider(names_from = Class, values_from = n, values_fill = 0) %>%
  arrange(PCB, PAH) %>%
  kable()
```

What that shows is that MOST samples include data from most parameters, but:  

1.  The CALCULATED variables are not consistently provided
2.  PCBs, and Dioxins and Furans, and PFCs are each more specialized, and less
    frequently sampled.

# Totals And Calculations
Many parameters are TOTALS or Calculated sums of related contaminants. These are
derived parameters which may be of special interest for summaries for the State
of Casco Bay report, but they are not primary analytic values.

These pose challenges in interpreting secondary data such as from EGAD because:
1.  Totals are not provided for all samples, potentially biasing results
2.  Methods for calculating totals are not provided in the available metadata
3.  Totals must be based on specific assumptions for handling non-detects.

## List of Total And Calculated Values
```{r}
SWAT_simplified %>%
  select(PARAMETER, `TEST METHOD`) %>%
  filter(grepl('TOTAL', PARAMETER, ignore.case = TRUE) |
         grepl('CALCULATED', `TEST METHOD`)) %>%
  group_by(`TEST METHOD`, PARAMETER) %>%
  summarize(.groups = 'drop') %>%
  rename(Parameter = PARAMETER, Test = `TEST METHOD`) %>%
  kable()
```

## Alternate Versions of Totals?
Many TOTAL parameters come in triplets -- with a suffix of "-D', or '-H' or
'-O'. It appears those are for totals calculated using different assumptions
about how to address non-detects, specifically, totals  with "-D" stands for
"detection limit", "-H" stands for "Half Detection Limit", and "-O" stands for
"Zero".

# Check if Data is Consistent with Our Interpretation
If our interpretation is correct, any total with -D is greater than or equal to
related Totals with -H, which in turn will be greater than any total with -O.

We can check that as follows:
```{r}
SWAT_simplified %>%
  filter (`WEIGHT BASIS` == 'WET') %>%
  select(Code, ANALYSIS_LAB_SAMPLE_ID, PARAMETER, CONCENTRATION) %>%
  filter(grepl('TOTAL', PARAMETER, ignore.case = TRUE)) %>%
  filter(grepl('-[DHO]$', PARAMETER)) %>%
  mutate(suffix = substr(PARAMETER, nchar(PARAMETER), nchar(PARAMETER)),
         prefix = substr(PARAMETER, 1, nchar(PARAMETER)-2)) %>%
  pivot_wider(c(Code, prefix, ANALYSIS_LAB_SAMPLE_ID),
              names_from = suffix, values_from = CONCENTRATION) %>%
  mutate(trouble = ! (O <=H & H <= D)) %>%
  filter(trouble)
```
So, we appear to be correct.

Note than none of these uses statistical procedures for estimating non-detect
values.  It would be nice to be able to use more modern approaches, but that
would require detailed understanding of how each of the totals were calculated.


# How Were Totals Calculated?
We have limited metadata showing us how specific calculated and total values
were calculated. It appears some detail is available in papers referenced in
recent SWAT reports, especially LeBlanc et al., 2009.

> Leblanc, L.A., Krahforst, C.F., Aub√©, J., Roach, S., Brun, G., Harding, G.,
  Hennigar, P., Page, D., Jones, S., Shaw, S., Stahlnecker, J., Schwartz, J.,
  Taylor, D., Thorpe, B., & Wells, P. (2009).
  Eighteenth Year of the Gulf of Maine Environmental Monitoring Program.
  Gulf of Maine Council on the Marine Environment.

Tables in that report appear to provide recipies for calculating several important data summaries. 

Analysis of PAH total and PCB totals took a lot of time and effort, so we have
broken them off into separate notebooks:
*  "SWAT_data_examinatin_PAH.Rmd"
*  "SWAT_data_examination_PCB.Rmd: